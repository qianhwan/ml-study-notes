{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision trees\n",
    "\n",
    "* Decision trees algorithm can be used for both classification and regression problems, we talk about decision trees in classification problems first.\n",
    "\n",
    "## What is it\n",
    "\n",
    "* A decision tree consists of decision blocks and terminating blocks, decision blocks can lead to other decision blocks or terminating blocks through branchs. Each decision blocks represents a feature and the value of the feature decides which branch to go to. Terminating blocks represent labels.\n",
    "\n",
    "#### Input\n",
    "1. A labeled dataset ***A***.\n",
    "2. An unlabeled datas sample ***x***.\n",
    "\n",
    "#### Output\n",
    "1. A decision tree ***T***.\n",
    "2. Label for ***x***.\n",
    "\n",
    "## How does it work\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "1. Information gain: building a decision tree is all about how to split the dataset, the change in information before and after the split is called the information gain. We split the dataset the way that it gives the highest information gain.\n",
    "\n",
    "2. Entropy (Shannon entropy): the way to calculation the information of a dataset.\n",
    "\n",
    "    H = -sum(p(x_i)log2p(x_i))\n",
    "    \n",
    "    where i = 1...n, and p(x_i) is the probability of choosing this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def shannon_entropy(dataset: list) -> float:\n",
    "    num_samples = len(dataset)\n",
    "    label_counts = {}\n",
    "    for sample in dataset:\n",
    "        label = sample[-1]\n",
    "        label_counts[label] = label_counts.get(label, 0) + 1\n",
    "    entropy = 0.0\n",
    "    for label, count in label_counts.items():\n",
    "        prob = count / num_samples\n",
    "        entropy -= prob * log(prob, 2)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_branch():\n",
    "    Check if every item in dataset is in the same class:\n",
    "        if so return class label\n",
    "        else\n",
    "            find the best feature to split the dataset\n",
    "            split the dataset and create a branch node\n",
    "            for each split\n",
    "                call create_branch and add result to branch node\n",
    "            return branch node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two components in the above pseudo code: split the dataset and find the best feature to split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset: list,\n",
    "                  index: int,\n",
    "                  value) -> list:\n",
    "    '''\n",
    "    :param list dataset: dataset to be splitted\n",
    "    :param int index: feature index, which feature are we splitting on\n",
    "    :param int/str/float value: feature value\n",
    "    :return: list of rest dataset\n",
    "    \n",
    "    e.g.\n",
    "    >>> myDat\n",
    "    [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]\n",
    "    >>> trees.splitDataSet(myDat,0,1)\n",
    "    [[1, 'yes'], [1, 'yes'], [0, 'no']]\n",
    "    >>> trees.splitDataSet(myDat,0,0)\n",
    "    [[1, 'no'], [1, 'no']]\n",
    "    '''\n",
    "    rest_dataset = []\n",
    "    for sample in dataset:\n",
    "        if sample[index] == value:\n",
    "            # exclude this feature\n",
    "            rest_feats = sample[:index]\n",
    "            rest_feats.extend(sample[index+1:])\n",
    "            # append this sample with reduced features\n",
    "            rest_dataset.append(rest_feats)\n",
    "    return rest_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_feature_to_split(dataset: list):\n",
    "    num_feats = len(dataset[0]) - 1\n",
    "    base_entropy = shannon_entropy(dataset)\n",
    "    best_info_gain = 0.0\n",
    "    best_feat_index = -1\n",
    "    for i in range(num_feats):\n",
    "        feat_values = [sample[i] for sample in dataset]\n",
    "        new_entropy = 0.0\n",
    "        for value in set(feat_values):\n",
    "            subset = split_dataset(dataset, i, value)\n",
    "            prob = len(subset) / len(dataset)\n",
    "            new_entropy += prob * shannon_entropy(subset)\n",
    "        info_gain = base_entropy - new_entropy\n",
    "        if info_gain > best_info_gain:\n",
    "            best_info_gain = info_gain\n",
    "            best_feat_index = i\n",
    "    return best_feat_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_count(lables):\n",
    "    lable_count = {}\n",
    "    for lable in lables:\n",
    "        lable_count[lable] = lable_count.get(lable, 0) + 1\n",
    "    sorted_label_count = [k for k, v in sorted(lable_count.items(),\n",
    "                                               key=lambda item: item[1],\n",
    "                                               reverse=True)]\n",
    "    return sorted_label_count[0]                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(dataset, features):\n",
    "    labels = [sample[-1] for sample in dataset]\n",
    "    if len(set(labels)) == 1:\n",
    "        # only one label exists\n",
    "        return labels[0]\n",
    "    if len(dataset[0]) == 1:\n",
    "        # no more features\n",
    "        return majority_count(dataset)\n",
    "    best_feat_index = best_feature_to_split(dataset)\n",
    "    best_feat = features[best_feat_index]\n",
    "    # new tree node\n",
    "    tree = {best_feat: {}}\n",
    "    features.remove(best_feat)\n",
    "    feat_values = [sample[best_feat_index] for sample in dataset]\n",
    "    for value in set(feat_values):\n",
    "        subfeatures = features[:]\n",
    "#         import pdb; pdb.set_trace()\n",
    "        tree[best_feat][value] = build_tree(split_dataset(dataset, best_feat_index, value), subfeatures)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    dataset = [[1, 1, 'yes'],\n",
    "               [1, 1, 'yes'],\n",
    "               [1, 0, 'no'],\n",
    "               [0, 1, 'no'],\n",
    "               [0, 1, 'no']]\n",
    "    features = ['no surfacing', 'flippers']\n",
    "    return dataset, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, features = create_dataset()\n",
    "tree = build_tree(dataset, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_decision_tree(tree, features, test_data):\n",
    "    root_feature = list(tree.keys())[0]\n",
    "    child_tree = tree[root_feature]\n",
    "    root_feature_index = features.index(root_feature)\n",
    "    for key in child_tree.keys():\n",
    "        if test_data[root_feature_index] == key:\n",
    "            if isinstance(child_tree[key], dict):\n",
    "                # not leaf yet\n",
    "                return classify_decision_tree(child_tree[key], features, test_data)\n",
    "            else:\n",
    "                return child_tree[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['no surfacing', 'flippers']\n",
    "classify_decision_tree(tree, features, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
